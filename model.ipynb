{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjDSbxIul0iV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_emb, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_emb, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_emb, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, head_size)\n",
        "    wei = q@k.transpose(-2, -1) # (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei) # (B, T, T)\n",
        "    v = self.value(x) # (B, T, head_size)\n",
        "    out = wei@v # (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "A6tl7JanmBvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_head, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    self.proj = nn.Linear(n_emb, n_emb)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    out = self.proj(out)\n",
        "    out = self.dropout(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "e_tMemfboJgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_emb):\n",
        "    super().__init__()\n",
        "    self.nets = nn.Sequential([\n",
        "        nn.Linear(n_emb, 4*n_emb),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_emb, n_emb),\n",
        "        nn.Dropout(dropout),\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.nets(x)"
      ],
      "metadata": {
        "id": "ZoNSGb1pYNsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_emb, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_emb//n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_emb)\n",
        "    self.ln1 = nn.LayerNorm(n_emb)\n",
        "    self.ln2 = nn.LayerNorm(n_emb)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "1p-4-POlZ1wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TinyGPT(nn.Module):\n",
        "\n",
        "  def __init__(self, voc_size, n_emb):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(voc_size, n_emb)\n",
        "    self.pos_embedding_table = nn.Embedding(block_size, n_emb)\n",
        "    self.blocks = nn.Sequential([\n",
        "        *[Block(n_emb, n_head) for _ in range(n_block)]\n",
        "    ])\n",
        "    self.ln_f = nn.LayerNorm(n_emb)\n",
        "    self.lm_head = nn.Linear(n_emb, voc_size)\n",
        "\n",
        "  def forward(self, idx, target=None):\n",
        "    B, T, C = idx.shape\n",
        "    token_embed = self.token_embedding_table(idx)\n",
        "    pos_embed = self.pos_embedding_table(torch.arange(T, device=device))\n",
        "    x = token_embed + pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.in_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if target is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      target = target.view(B*T)\n",
        "      loss = F.cross_entropy(logits, target)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens):\n",
        "    for _ in range(max_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_cond) # logits (B, T, C)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, n_sample=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "QtDCwRXobC_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}